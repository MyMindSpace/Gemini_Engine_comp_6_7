{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "206131eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment and Imports\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "# Import our LSTM components\n",
    "from comp5.config.settings import LSTMConfig, AstraDBConfig\n",
    "from comp5.core.gate_networks import LSTMGateNetwork\n",
    "from comp5.core.memory_manager import MemoryManager\n",
    "from comp5.models.memory_item import MemoryItem\n",
    "from comp5.models.rl_experience import RLExperience, RLState, RLAction, RLReward\n",
    "from comp5.database.astra_connector import AstraDBConnector\n",
    "from comp5.rl_training.gate_optimizer import GateOptimizer\n",
    "from comp5.rl_training.reward_calculator import RewardCalculator\n",
    "from comp5.utils.similarity import cosine_similarity\n",
    "from comp5.utils.token_counter import estimate_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed8478",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATABASE_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memory_manager, rl_trainer, config\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m memory_manager, rl_trainer, config = \u001b[38;5;28;01mawait\u001b[39;00m initialize_lstm_system()\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ LSTM system initialized successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Gate thresholds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory_manager.gate_network.get_thresholds()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36minitialize_lstm_system\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create configuration\u001b[39;00m\n\u001b[32m      6\u001b[39m config = LSTMConfig()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m config.astra_db.endpoint = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mDATABASE_ID\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREGION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.apps.astra.datastax.com\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m config.astra_db.token = TOKEN\n\u001b[32m      9\u001b[39m config.astra_db.keyspace = KEYSPACE\n",
      "\u001b[31mNameError\u001b[39m: name 'DATABASE_ID' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d691cd26",
   "metadata": {},
   "source": [
    "## Component 5 gets data from Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a46bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from uuid import UUID\n",
    "from dotenv import load_dotenv\n",
    "from astrapy import DataAPIClient\n",
    "\n",
    "# Load env vars\n",
    "load_dotenv()\n",
    "\n",
    "user_id_fs = \"123\"\n",
    "API_ENDPOINT = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "API_TOKEN = os.getenv(\"ASTRA_DB_TOKEN\")\n",
    "KEYSPACE = os.getenv(\"KEYSPACE\", \"default_keyspace\")\n",
    "COLLECTION = os.getenv(\"COLLECTION\", \"chat_embeddings\")   # or \"memory_embeddings\" if that's created\n",
    "\n",
    "# Connect client\n",
    "client = DataAPIClient(API_TOKEN)\n",
    "db = client.get_database_by_api_endpoint(API_ENDPOINT)\n",
    "collection = db.get_collection(COLLECTION, keyspace=KEYSPACE)\n",
    "\n",
    "# Query memory embeddings for user_id \"123\"\n",
    "# Modify the query syntax if needed, depending on how your records are stored\n",
    "query = {\"user_id\": user_id_fs}  # Filter records\n",
    "\n",
    "docs = collection.find(query)\n",
    "\n",
    "formatted_results = []\n",
    "\n",
    "for doc in docs:\n",
    "    # Remap/transform as needed for the target schema\n",
    "    result = {\n",
    "        \"id\": str(doc.get('id', '')),  # Make sure this is UUID format in your DB\n",
    "        \"user_id\": doc.get('user_id', ''),\n",
    "        \"memory_type\": doc.get('memory_type', ''),\n",
    "        \"content_summary\": doc.get('content_summary', ''),\n",
    "        \"importance_score\": doc.get('importance_score', 0.0),\n",
    "        \"gate_scores\": json.dumps(doc.get('gate_scores', {})),  # ensure JSON string\n",
    "        \"feature_vector\": doc.get('feature_vector', []),\n",
    "        \"created_at\": doc.get('created_at', ''),\n",
    "        \"last_accessed\": doc.get('last_accessed', ''),\n",
    "        \"access_frequency\": doc.get('access_frequency', 0),\n",
    "        \"emotional_significance\": doc.get('emotional_significance', 0.0),\n",
    "        \"temporal_relevance\": doc.get('temporal_relevance', 0.0),\n",
    "        \"relationships\": doc.get('relationships', []),\n",
    "        \"context_needed\": json.dumps(doc.get('context_needed', {})),\n",
    "        \"retrieval_triggers\": doc.get('retrieval_triggers', []),\n",
    "        \"original_entry_id\": str(doc.get('original_entry_id', '')),\n",
    "    }\n",
    "    formatted_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddb5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 05:39:55,093 - comp5.core.memory_manager - INFO - Initializing Memory Manager...\n",
      "2025-09-03 05:39:55,093 - comp5.database.astra_connector - INFO - Connecting to Astra DB: https://a240df36-45c0-4e1a-a227-2c9a3b1b4822-us-east1.apps.astra.datastax.com\n",
      "2025-09-03 05:39:55,286 - astrapy.data.database - INFO - findCollections\n",
      "2025-09-03 05:39:56,457 - astrapy.data.database - INFO - finished findCollections\n",
      "2025-09-03 05:39:56,457 - comp5.database.astra_connector - INFO - Connected! Found collections: ['chat_embeddings', 'vector_embeddings']\n",
      "2025-09-03 05:39:57,638 - comp5.database.astra_connector - INFO - Successfully connected to Astra DB\n",
      "2025-09-03 05:39:57,648 - comp5.core.memory_manager - INFO - No existing gate network found, using fresh model\n",
      "2025-09-03 05:39:57,648 - comp5.core.memory_manager - INFO - Memory Manager initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LSTM system initialized successfully\n",
      "üìä Gate thresholds: {'forget': 0.30000001192092896, 'input': 0.4000000059604645, 'output': 0.4000000059604645}\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize LSTM System\n",
    "async def initialize_lstm_system():\n",
    "    \"\"\"Initialize the complete LSTM memory system\"\"\"\n",
    "    \n",
    "    # Create configuration\n",
    "    config = LSTMConfig()\n",
    "    config.astra_db.endpoint = API_ENDPOINT\n",
    "    config.astra_db.token = API_TOKEN\n",
    "    config.astra_db.keyspace = KEYSPACE\n",
    "    \n",
    "    # Initialize memory manager\n",
    "    memory_manager = MemoryManager(config)\n",
    "    await memory_manager.initialize()\n",
    "    \n",
    "    # Initialize RL trainer\n",
    "    rl_trainer = GateOptimizer(\n",
    "        gate_network=memory_manager.gate_network,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    return memory_manager, rl_trainer, config\n",
    "\n",
    "# Run initialization\n",
    "memory_manager, rl_trainer, config = await initialize_lstm_system()\n",
    "print(\"‚úÖ LSTM system initialized successfully\")\n",
    "print(f\"üìä Gate thresholds: {memory_manager.gate_network.get_thresholds()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a068a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 05:57:36,590 - comp5.core.memory_manager - INFO - Processing new entry for user 123\n",
      "2025-09-03 05:57:36,591 - comp5.core.memory_manager - INFO - Input gate score: 0.646, decision: True\n",
      "2025-09-03 05:57:36,592 - comp5.core.memory_manager - ERROR - Error processing new entry for user 123: 'dict' object has no attribute 'lower'\n",
      "2025-09-03 05:57:36,592 - comp5.core.memory_manager - INFO - Processing new entry for user 123\n",
      "2025-09-03 05:57:36,593 - comp5.core.memory_manager - INFO - Input gate score: 1.000, decision: True\n",
      "2025-09-03 05:57:36,595 - comp5.core.memory_manager - ERROR - Error processing new entry for user 123: 'dict' object has no attribute 'lower'\n",
      "2025-09-03 05:57:36,595 - comp5.core.memory_manager - INFO - Processing new entry for user 123\n",
      "2025-09-03 05:57:36,597 - comp5.core.memory_manager - INFO - Input gate score: 0.608, decision: True\n",
      "2025-09-03 05:57:36,598 - comp5.core.memory_manager - ERROR - Error processing new entry for user 123: 'dict' object has no attribute 'lower'\n",
      "2025-09-03 05:57:36,598 - comp5.core.memory_manager - INFO - Processing new entry for user 123\n",
      "2025-09-03 05:57:36,599 - comp5.core.memory_manager - INFO - Input gate score: 0.675, decision: True\n",
      "2025-09-03 05:57:36,600 - comp5.core.memory_manager - ERROR - Error processing new entry for user 123: 'dict' object has no attribute 'lower'\n",
      "2025-09-03 05:57:36,601 - comp5.core.memory_manager - INFO - Processing new entry for user 123\n",
      "2025-09-03 05:57:36,602 - comp5.core.memory_manager - INFO - Input gate score: 0.620, decision: True\n",
      "2025-09-03 05:57:36,603 - comp5.core.memory_manager - ERROR - Error processing new entry for user 123: 'dict' object has no attribute 'lower'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Processing journal entries through LSTM gates...\n",
      "{'id': 'bfd617c0-8ca4-4c9c-ada3-09ff95ab3819', 'user_id': '123', 'memory_type': 'conversation', 'content_summary': 'I had my English exam today. I have a date with Sarah tomorrow', 'importance_score': 0.3504100122237817, 'gate_scores': '{\"forget_score\": 0.5, \"input_score\": 0.35589662194252014, \"output_score\": 0.32, \"confidence\": 0.7039125472307205}', 'feature_vector': [0.370590478181839, 0.9829629063606262, 0.5, 1, 0, 0.5, 1, 1, 0.75, 0.699999988079071, 0.5249999761581421, 0.5, 0.5, 0.9791666269302368, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 0.5714285969734192, 1, 0.75, 0.699999988079071, 0.5, 0.3630714416503906, 0.7072107195854187, 0, 0.6042777895927429, 0.9751241207122803, 0.01113926898688078, 0.505569577217102, 0.5000373721122742, 1, 0.01113926898688078, 0.01113926898688078, 0.01113926898688078, 0.01113926898688078, 0.01113926898688078, 0.01113926898688078, 0.01113926898688078, 0.2222352772951126, 0.02227853797376156, 0.6897003054618835, 0.3298759162425995, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.5, 0.3199999928474426, 0.6800000071525574, 0.25641027092933655, 0.25641027092933655, 0.7179487347602844, 0.10000000149011612, 0.1428571492433548, 0, 0.8199999928474426, 0, 0, 1, 0, 0, 0.1599999964237213, 0.2176000028848648, 0.5128205418586731, 0.1428571492433548, 0.5, 0.5, 0.5, 0.5, 0.5, 0, 0.699999988079071, 0.03999999910593033, 0.10000000149011612, 0, 0.18000000715255737, 0.5, 0.5, 0.10999999940395355, 0.05000000074505806, 1], 'created_at': '2025-09-01T23:31:58.225496', 'last_accessed': '2025-09-01T23:31:58.225496', 'access_frequency': 0, 'emotional_significance': 0.26570209860801697, 'temporal_relevance': 0.81, 'relationships': [], 'context_needed': '{\"user_id\": \"123\", \"session_id\": \"session_9f3a5ac9\", \"timestamp\": \"2025-09-01T23:31:56.623616\"}', 'retrieval_triggers': ['joy', 'today.', 'best.', 'exam', 'english', 'nervous'], 'original_entry_id': 'robust_test_entry_001'}\n",
      "‚ùå Entry not stored (low importance)\n",
      "{'id': 'demo_memory_001', 'user_id': '123', 'memory_type': 'event', 'content_summary': 'I had my English exam today. I was nervous but I did my best. I got 90% in the exam. I am happy with my result. I have a date with Ninad tomorrow. I am excited to see him.', 'importance_score': 0.85, 'gate_scores': '{\"forget_score\": 0.8, \"input_score\": 0.9, \"output_score\": 0.85, \"confidence\": 0.88}', 'feature_vector': [0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001, 0.7000000000000001, 0.8, 0.9, 1, 1.1, 1.2000000000000002, 1.3, 1.4000000000000001, 1.5, 1.6, 1.7000000000000002, 1.8, 1.9000000000000001, 2, 2.1, 2.2, 2.3000000000000003, 2.4000000000000004, 2.5, 2.6, 2.7, 2.8000000000000003, 2.9000000000000004, 3, 3.1, 3.2, 3.3000000000000003, 3.4000000000000004, 3.5, 3.6, 3.7, 3.8000000000000003, 3.9000000000000004, 4, 4.1000000000000005, 4.2, 4.3, 4.4, 4.5, 4.6000000000000005, 4.7, 4.800000000000001, 4.9, 5, 5.1000000000000005, 5.2, 5.300000000000001, 5.4, 5.5, 5.6000000000000005, 5.7, 5.800000000000001, 5.9, 6, 6.1000000000000005, 6.2, 6.300000000000001, 6.4, 6.5, 6.6000000000000005, 6.7, 6.800000000000001, 6.9, 7, 7.1000000000000005, 7.2, 7.300000000000001, 7.4, 7.5, 7.6000000000000005, 7.7, 7.800000000000001, 7.9, 8, 8.1, 8.200000000000001, 8.3, 8.4, 8.5, 8.6, 8.700000000000001, 8.8, 8.9], 'created_at': '2025-09-01T23:12:03.272336', 'last_accessed': '2025-09-01T23:12:03.272336', 'access_frequency': 0, 'emotional_significance': 0.9, 'temporal_relevance': 0.95, 'relationships': [], 'context_needed': '{\"user_id\": \"123\", \"memory_type\": \"academic_achievement\"}', 'retrieval_triggers': ['exam', 'English', 'test', 'nervous', 'score', '90%', 'Ninad', 'date', 'tomorrow', 'excited', 'happy', 'result'], 'original_entry_id': 'first_ds_entry_001'}\n",
      "‚ùå Entry not stored (low importance)\n",
      "{'id': '6b62f5cd-21ac-4b6d-8988-edac9c5d886d', 'user_id': '123', 'memory_type': 'conversation', 'content_summary': 'I had my English exam today. I have a date with Ninad tomorrow', 'importance_score': 0.3472166725183145, 'gate_scores': '{\"forget_score\": 0.5, \"input_score\": 0.3452521562576294, \"output_score\": 0.32, \"confidence\": 0.696215996146202}', 'feature_vector': [0.370590478181839, 0.9829629063606262, 0.5, 1, 0, 0.5, 1, 1, 0.75, 0.699999988079071, 0.5249999761581421, 0.5, 0.5, 0.9791666269302368, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 0.5714285969734192, 1, 0.75, 0.699999988079071, 0.5, 0.34850236773490906, 0.6977240443229675, 0, 0.6081030964851379, 0.9760829210281372, 0.004964055493474007, 0.5024820566177368, 0.5000542402267456, 1, 0.004964055493474007, 0.004964055493474007, 0.004964055493474007, 0.004964055493474007, 0.004964055493474007, 0.004964055493474007, 0.004964055493474007, 0.21319910883903503, 0.009928110986948013, 0.6810725331306458, 0.32217785716056824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.5, 0.3199999928474426, 0.6800000071525574, 0.25641027092933655, 0.25641027092933655, 0.7179487347602844, 0.10000000149011612, 0.1428571492433548, 0, 0.8199999928474426, 0, 0, 1, 0, 0, 0.1599999964237213, 0.2176000028848648, 0.5128205418586731, 0.1428571492433548, 0.5, 0.5, 0.5, 0.5, 0.5, 0, 0.699999988079071, 0.03999999910593033, 0, 0, 0.18000000715255737, 0.5, 0.5, 0.10999999940395355, 0, 1], 'created_at': '2025-09-01T23:28:46.220915', 'last_accessed': '2025-09-01T23:28:46.220915', 'access_frequency': 0, 'emotional_significance': 0.26082709431648254, 'temporal_relevance': 0.81, 'relationships': [], 'context_needed': '{\"user_id\": \"123\", \"session_id\": \"session_b23399f9\", \"timestamp\": \"2025-09-01T23:28:44.640170\"}', 'retrieval_triggers': ['best.', 'nervous', 'today.', 'joy', 'exam', 'english'], 'original_entry_id': 'robust_test_entry_001'}\n",
      "‚ùå Entry not stored (low importance)\n",
      "{'id': '293600aa-98d0-4bc2-b76d-c541d0008f4d', 'user_id': '123', 'memory_type': 'conversation', 'content_summary': 'I had my English exam today. I have a date with Ninad tomorrow', 'importance_score': 0.34861767969376006, 'gate_scores': '{\"forget_score\": 0.5, \"input_score\": 0.34992218017578125, \"output_score\": 0.32, \"confidence\": 0.696215996146202}', 'feature_vector': [0.370590478181839, 0.9829629063606262, 0.5, 1, 0, 0.5, 1, 1, 0.75, 0.699999988079071, 0.5249999761581421, 0.5, 0.5, 0.9791666269302368, 0.5, 1, 0.5, 0.5, 0.5, 0.5, 0.5714285969734192, 1, 0.75, 0.699999988079071, 0.5, 0.3656530976295471, 0.7035671472549438, 0, 0.6194207072257996, 0.9776296019554138, 0.02419847995042801, 0.5120992660522461, 0.49292677640914917, 1, 0.02419847995042801, 0.02419847995042801, 0.02419847995042801, 0.02419847995042801, 0.02419847995042801, 0.02419847995042801, 0.02419847995042801, 0.23247994482517242, 0.04839695990085602, 0.687992513179779, 0.34449025988578796, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0.5, 0.3199999928474426, 0.6800000071525574, 0.25641027092933655, 0.25641027092933655, 0.7179487347602844, 0.10000000149011612, 0.1428571492433548, 0, 0.8199999928474426, 0, 0, 1, 0, 0, 0.1599999964237213, 0.2176000028848648, 0.5128205418586731, 0.1428571492433548, 0.5, 0.5, 0.5, 0.5, 0.5, 0, 0.699999988079071, 0.03999999910593033, 0, 0, 0.18000000715255737, 0.5, 0.5, 0.10999999940395355, 0, 1], 'created_at': '2025-09-01T23:28:03.457975', 'last_accessed': '2025-09-01T23:28:03.457975', 'access_frequency': 0, 'emotional_significance': 0.2694050073623657, 'temporal_relevance': 0.81, 'relationships': [], 'context_needed': '{\"user_id\": \"123\", \"session_id\": \"session_f71ebd9f\", \"timestamp\": \"2025-09-01T23:28:01.765745\"}', 'retrieval_triggers': ['today.', 'english', 'joy', 'exam', 'nervous', 'best.'], 'original_entry_id': 'robust_test_entry_001'}\n",
      "‚ùå Entry not stored (low importance)\n",
      "{'id': 'c198513d-e9c9-4c0b-9aa3-2c303fe1a277', 'user_id': '123', 'memory_type': 'event', 'content_summary': 'I had my English exam today. I have a date with Sarah tomorrow', 'importance_score': 0.34777318618542113, 'gate_scores': '{\"forget_score\": 0.5, \"input_score\": 0.3471072018146515, \"output_score\": 0.32, \"confidence\": 0.703903979063034}', 'feature_vector': [0.25, 0.0669873058795929, 0.8909157514572144, 0.8117449283599854, 0, 0.5, 0.5, 1, 1, 0.8999999761581421, 0.7250000238418579, 0.5, 0.5, 0.7916666269302368, 0.5714285969734192, 0.5, 0.5, 0.5, 1, 0.5, 0.5, 1, 1, 0.8999999761581421, 0.5, 0.35306257009506226, 0.7066048383712769, 0, 0.6148240566253662, 0.9749044179916382, 0.009121494367718697, 0.5045607686042786, 0.4973677098751068, 1, 0.009121494367718697, 0.009121494367718697, 0.009121494367718697, 0.009121494367718697, 0.009121494367718697, 0.009121494367718697, 0.009121494367718697, 0.2193652242422104, 0.018242988735437393, 0.6889399290084839, 0.3287099003791809, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0.5, 0.3199999928474426, 0.6800000071525574, 0.25641027092933655, 1, 0.7179487347602844, 0.10000000149011612, 0.1428571492433548, 0, 0.8199999928474426, 0, 1, 1, 0, 0, 0.1599999964237213, 0.2176000028848648, 1, 0.1428571492433548, 0.5, 0.5, 0.5, 0.5, 0.5, 0, 0.699999988079071, 0.03999999910593033, 0.10000000149011612, 0.30000001192092896, 0.18000000715255737, 0.5, 0.5, 0.10999999940395355, 0.20000000298023224, 1], 'created_at': '2025-09-02T14:25:04.443839', 'last_accessed': '2025-09-02T14:25:04.443839', 'access_frequency': 0, 'emotional_significance': 0.26386019587516785, 'temporal_relevance': 0.89, 'relationships': [], 'context_needed': '{\"user_id\": \"123\", \"session_id\": \"session_1352a9ca\", \"timestamp\": \"2025-09-02T14:25:00.537037\"}', 'retrieval_triggers': ['exam', 'today.', 'english', 'best.', 'nervous', 'joy'], 'original_entry_id': 'robust_test_entry_001'}\n",
      "‚ùå Entry not stored (low importance)\n",
      "\n",
      "üìä Results: 0/5 entries became memories\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Sample journal entries\n",
    "sample_entries = formatted_results\n",
    "\n",
    "# Process entries through LSTM system\n",
    "print(\"üìù Processing journal entries through LSTM gates...\")\n",
    "created_memories = []\n",
    "\n",
    "for i, entry in enumerate(sample_entries, 1):\n",
    "    print(entry)\n",
    "    # Generate dummy feature vector (replace with Component 4)\n",
    "    feature_vector = entry['feature_vector']\n",
    "    \n",
    "    # Generate dummy embeddings (replace with Component 3)  \n",
    "    embeddings = [random.random() for _ in range(768)]\n",
    "    \n",
    "    # Process through memory manager\n",
    "    memory = await memory_manager.process_new_entry(\n",
    "        user_id='123',\n",
    "        feature_vector=feature_vector,\n",
    "        content=entry,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "    if memory:\n",
    "        created_memories.append(memory)\n",
    "        print(f\"‚úÖ Memory created: {memory.memory_type} (importance: {memory.importance_score:.3f})\")\n",
    "    else:\n",
    "        print(\"‚ùå Entry not stored (low importance)\")\n",
    "\n",
    "print(f\"\\nüìä Results: {len(created_memories)}/{len(sample_entries)} entries became memories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02122717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 06:00:20,589 - comp5.core.memory_manager - INFO - Assembling context for user 123\n",
      "2025-09-03 06:00:20,590 - astrapy.data.cursors.cursor - INFO - cursor fetching a page: (empty page state) from memory_embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing memory context retrieval...\n",
      "\n",
      "‚ùì Query: 'How's work going?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 06:00:21,770 - astrapy.utils.api_commander - WARNING - APICommander about to raise from: [{'message': 'Collection does not exist, collection name: memory_embeddings', 'errorCode': 'COLLECTION_NOT_EXIST', 'id': '9ae76c7a-8292-4c86-87bc-2e1d28c54f78', 'family': 'REQUEST', 'title': 'Collection does not exist, collection name', 'scope': 'EMPTY'}]\n",
      "2025-09-03 06:00:21,771 - comp5.database.astra_connector - ERROR - Error getting user memories: Collection does not exist, collection name: Collection does not exist, collection name: memory_embeddings (COLLECTION_NOT_EXIST)\n",
      "2025-09-03 06:00:21,771 - comp5.database.memory_store - INFO - Retrieved 0 memories for user 123\n",
      "2025-09-03 06:00:21,772 - comp5.core.memory_manager - INFO - No memories found for user 123\n",
      "2025-09-03 06:00:21,772 - comp5.core.memory_manager - INFO - Assembling context for user 123\n",
      "2025-09-03 06:00:21,773 - comp5.core.memory_manager - INFO - No memories found for user 123\n",
      "2025-09-03 06:00:21,773 - comp5.core.memory_manager - INFO - Assembling context for user 123\n",
      "2025-09-03 06:00:21,774 - comp5.core.memory_manager - INFO - No memories found for user 123\n",
      "2025-09-03 06:00:21,774 - comp5.core.memory_manager - INFO - Assembling context for user 123\n",
      "2025-09-03 06:00:21,775 - comp5.core.memory_manager - INFO - No memories found for user 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Found 0 relevant memories\n",
      "üìä Metadata: {'reason': 'no_memories_found'}\n",
      "\n",
      "‚ùì Query: 'Tell me about AI'\n",
      "üéØ Found 0 relevant memories\n",
      "üìä Metadata: {'reason': 'no_memories_found'}\n",
      "\n",
      "‚ùì Query: 'Any upcoming appointments?'\n",
      "üéØ Found 0 relevant memories\n",
      "üìä Metadata: {'reason': 'no_memories_found'}\n",
      "\n",
      "‚ùì Query: 'How am I feeling lately?'\n",
      "üéØ Found 0 relevant memories\n",
      "üìä Metadata: {'reason': 'no_memories_found'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test Context Retrieval\n",
    "print(\"üîç Testing memory context retrieval...\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"How's work going?\",\n",
    "    \"Tell me about AI\",\n",
    "    \"Any upcoming appointments?\",\n",
    "    \"How am I feeling lately?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n‚ùì Query: '{query}'\")\n",
    "    \n",
    "    # Generate query features (placeholder)\n",
    "    query_features = [random.random() for _ in range(90)]\n",
    "    \n",
    "    # Get relevant context\n",
    "    relevant_memories, metadata = await memory_manager.get_relevant_context(\n",
    "        user_id=\"123\",\n",
    "        query=query,\n",
    "        query_features=query_features,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Found {len(relevant_memories)} relevant memories\")\n",
    "    print(f\"üìä Metadata: {metadata}\")\n",
    "    \n",
    "    if relevant_memories:\n",
    "        for memory in relevant_memories[:2]:  # Show top 2\n",
    "            print(f\"   üí≠ {memory.memory_type}: {memory.content_summary[:60]}...\")\n",
    "            print(f\"      Importance: {memory.importance_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e92cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: RL Training Setup\n",
    "print(\"üéì Setting up RL training...\")\n",
    "\n",
    "# Create reward calculator\n",
    "reward_calc = RewardCalculator()\n",
    "\n",
    "# Simulate user feedback for training\n",
    "def simulate_user_interaction(memories_used: List[MemoryItem]) -> Dict:\n",
    "    \"\"\"Simulate user interaction data\"\"\"\n",
    "    return {\n",
    "        'response_length': random.randint(50, 200),\n",
    "        'has_follow_up_questions': random.choice([True, False]),\n",
    "        'session_duration_minutes': random.uniform(2, 15),\n",
    "        'positive_feedback': random.choice([True, False]),\n",
    "        'context_relevance_score': random.uniform(0.3, 1.0),\n",
    "        'memories_used': len(memories_used),\n",
    "        'processing_time_seconds': random.uniform(0.5, 2.0)\n",
    "    }\n",
    "\n",
    "# Test reward calculation\n",
    "print(\"üèÜ Testing reward calculation...\")\n",
    "\n",
    "for i in range(3):\n",
    "    # Simulate interaction\n",
    "    interaction_data = simulate_user_interaction(created_memories[:3])\n",
    "    \n",
    "    # Calculate reward using our reward calculator\n",
    "    reward = reward_calc.calculate_simple_reward({\n",
    "        'user_engagement': random.uniform(0.4, 1.0),\n",
    "        'context_relevance': random.uniform(0.3, 1.0), \n",
    "        'memory_efficiency': random.uniform(0.5, 1.0),\n",
    "        'user_satisfaction': random.uniform(0.4, 1.0),\n",
    "        'conversation_quality': random.uniform(0.5, 1.0)\n",
    "    })\n",
    "    \n",
    "    print(f\"üéØ Interaction {i+1}: Reward = {reward:.3f}\")\n",
    "    print(f\"   Engagement metrics: {interaction_data}\")\n",
    "\n",
    "print(\"‚úÖ Reward calculation working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b5933",
   "metadata": {},
   "source": [
    "## Component 5 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b942278",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mastrapy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataAPIClient\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Add comp5 to Python path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m project_root = os.path.dirname(os.path.abspath(\u001b[34;43m__file__\u001b[39;49m))\n\u001b[32m     29\u001b[39m comp5_path = os.path.join(project_root, \u001b[33m'\u001b[39m\u001b[33mcomp5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, comp5_path)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# Component 5 Pipeline - LSTM Memory Gates Processing\n",
    "# ==================================================\n",
    "\n",
    "# This script processes retrieved Astra DB data through Component 5's memory gates system:\n",
    "# 1. Retrieves memories from Astra DB for user_id=\"123\"\n",
    "# 2. Processes through LSTM gates (Input, Forget, Output)\n",
    "# 3. Applies intelligent memory filtering and scoring\n",
    "# 4. Outputs processed memories ready for Component 6\n",
    "\n",
    "# Architecture:\n",
    "# Astra DB ‚Üí Feature Processing ‚Üí LSTM Gates ‚Üí Memory Filtering ‚Üí Component 6 Input\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import json\n",
    "# import asyncio\n",
    "# import numpy as np\n",
    "# from typing import List, Dict, Any, Optional, Tuple\n",
    "# from datetime import datetime, timedelta\n",
    "# from uuid import UUID\n",
    "# from dotenv import load_dotenv\n",
    "# from astrapy import DataAPIClient\n",
    "\n",
    "# # Add comp5 to Python path\n",
    "# project_root = os.path.dirname(os.path.abspath(__file__))\n",
    "# comp5_path = os.path.join(project_root, 'comp5')\n",
    "# sys.path.insert(0, comp5_path)\n",
    "\n",
    "# # Component 5 imports (simplified for pipeline)\n",
    "# from comp5_lstm_config import LSTMConfig\n",
    "# from simple_lstm_gates import SimpleLSTMGates\n",
    "\n",
    "# # Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# class Component5Pipeline:\n",
    "#     \"\"\"\n",
    "#     Complete Component 5 processing pipeline\n",
    "#     Handles memory retrieval, gate processing, and intelligent filtering\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.config = self._create_config()\n",
    "#         self.gate_network = None\n",
    "#         self.memory_manager = None\n",
    "#         self.astra_client = None\n",
    "#         self.collection = None\n",
    "        \n",
    "#         # Processing statistics\n",
    "#         self.stats = {\n",
    "#             'memories_retrieved': 0,\n",
    "#             'memories_processed': 0,\n",
    "#             'input_gate_accepted': 0,\n",
    "#             'forget_gate_triggered': 0,\n",
    "#             'output_gate_filtered': 0,\n",
    "#             'final_memories_count': 0,\n",
    "#             'processing_time': 0.0\n",
    "#         }\n",
    "    \n",
    "#     def _create_config(self) -> LSTMConfig:\n",
    "#         \"\"\"Create Component 5 configuration from environment\"\"\"\n",
    "        \n",
    "#         # Astra DB config\n",
    "#         astra_config = AstraDBConfig(\n",
    "#             endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\", \"\"),\n",
    "#             token=os.getenv(\"ASTRA_DB_TOKEN\", \"\"),\n",
    "#             keyspace=os.getenv(\"KEYSPACE\", \"default_keyspace\")\n",
    "#         )\n",
    "        \n",
    "#         # Gate network config\n",
    "#         gate_config = GateNetworkConfig(\n",
    "#             input_size=90,  # Feature vector size from Component 4\n",
    "#             hidden_size=128,\n",
    "#             dropout_rate=0.1\n",
    "#         )\n",
    "        \n",
    "#         # Memory config\n",
    "#         memory_config = MemoryConfig(\n",
    "#             forget_threshold=0.3,\n",
    "#             input_threshold=0.4,  # 40% acceptance rate\n",
    "#             output_threshold=0.4,\n",
    "#             max_context_tokens=2000,\n",
    "#             max_memories_per_context=20\n",
    "#         )\n",
    "        \n",
    "#         return LSTMConfig(\n",
    "#             astra_db=astra_config,\n",
    "#             gate_network=gate_config,\n",
    "#             memory=memory_config\n",
    "#         )\n",
    "    \n",
    "#     async def initialize(self):\n",
    "#         \"\"\"Initialize the Component 5 pipeline\"\"\"\n",
    "#         print(\"üöÄ Initializing Component 5 Pipeline...\")\n",
    "        \n",
    "#         # Initialize Astra DB connection\n",
    "#         await self._init_astra_connection()\n",
    "        \n",
    "#         # Initialize gate networks\n",
    "#         self.gate_network = LSTMGateNetwork(\n",
    "#             input_size=self.config.gate_network.input_size,\n",
    "#             hidden_size=self.config.gate_network.hidden_size,\n",
    "#             dropout=self.config.gate_network.dropout_rate\n",
    "#         )\n",
    "        \n",
    "#         # Try to load pre-trained model\n",
    "#         try:\n",
    "#             model_path = os.path.join(comp5_path, \"models\", \"gate_network.pt\")\n",
    "#             if os.path.exists(model_path):\n",
    "#                 self.gate_network = LSTMGateNetwork.load_model(model_path)\n",
    "#                 print(\"‚úÖ Loaded pre-trained gate network\")\n",
    "#             else:\n",
    "#                 print(\"üìù Using fresh gate network (no pre-trained model found)\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è Could not load pre-trained model: {e}\")\n",
    "        \n",
    "#         # Initialize memory manager\n",
    "#         self.memory_manager = MemoryManager(self.config)\n",
    "        \n",
    "#         print(\"‚úÖ Component 5 Pipeline initialized successfully\")\n",
    "    \n",
    "#     async def _init_astra_connection(self):\n",
    "#         \"\"\"Initialize Astra DB connection\"\"\"\n",
    "#         api_endpoint = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "#         api_token = os.getenv(\"ASTRA_DB_TOKEN\")\n",
    "#         keyspace = os.getenv(\"KEYSPACE\", \"default_keyspace\")\n",
    "#         collection_name = os.getenv(\"COLLECTION\", \"chat_embeddings\")\n",
    "        \n",
    "#         if not api_endpoint or not api_token:\n",
    "#             raise ValueError(\"Missing ASTRA_DB_API_ENDPOINT or ASTRA_DB_TOKEN\")\n",
    "        \n",
    "#         # Connect to Astra DB\n",
    "#         self.astra_client = DataAPIClient(api_token)\n",
    "#         db = self.astra_client.get_database_by_api_endpoint(api_endpoint)\n",
    "#         self.collection = db.get_collection(collection_name, keyspace=keyspace)\n",
    "        \n",
    "#         print(f\"‚úÖ Connected to Astra DB: {collection_name}\")\n",
    "    \n",
    "#     async def retrieve_user_memories(self, user_id: str) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"Retrieve memories from Astra DB for the specified user\"\"\"\n",
    "#         print(f\"üìä Retrieving memories for user: {user_id}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Query Astra DB\n",
    "#             query = {\"user_id\": user_id}\n",
    "#             docs = list(self.collection.find(query))\n",
    "            \n",
    "#             formatted_memories = []\n",
    "#             for doc in docs:\n",
    "#                 # Convert Astra DB document to Component 5 format\n",
    "#                 memory = {\n",
    "#                     \"id\": str(doc.get('_id', '')),\n",
    "#                     \"user_id\": doc.get('user_id', ''),\n",
    "#                     \"memory_type\": doc.get('memory_type', 'conversation'),\n",
    "#                     \"content_summary\": doc.get('content_summary', ''),\n",
    "#                     \"importance_score\": float(doc.get('importance_score', 0.0)),\n",
    "#                     \"gate_scores\": doc.get('gate_scores', {}),\n",
    "#                     \"feature_vector\": doc.get('feature_vector', []),\n",
    "#                     \"created_at\": doc.get('created_at', ''),\n",
    "#                     \"last_accessed\": doc.get('last_accessed', ''),\n",
    "#                     \"access_frequency\": int(doc.get('access_frequency', 0)),\n",
    "#                     \"emotional_significance\": float(doc.get('emotional_significance', 0.0)),\n",
    "#                     \"temporal_relevance\": float(doc.get('temporal_relevance', 0.0)),\n",
    "#                     \"relationships\": doc.get('relationships', []),\n",
    "#                     \"context_needed\": doc.get('context_needed', {}),\n",
    "#                     \"retrieval_triggers\": doc.get('retrieval_triggers', []),\n",
    "#                     \"original_entry_id\": str(doc.get('original_entry_id', ''))\n",
    "#                 }\n",
    "#                 formatted_memories.append(memory)\n",
    "            \n",
    "#             self.stats['memories_retrieved'] = len(formatted_memories)\n",
    "#             print(f\"‚úÖ Retrieved {len(formatted_memories)} memories from Astra DB\")\n",
    "#             return formatted_memories\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error retrieving memories: {e}\")\n",
    "#             return []\n",
    "    \n",
    "#     def _ensure_feature_vector(self, memory: Dict[str, Any]) -> List[float]:\n",
    "#         \"\"\"Ensure memory has a valid 90-dimension feature vector\"\"\"\n",
    "#         feature_vector = memory.get('feature_vector', [])\n",
    "        \n",
    "#         if not feature_vector or len(feature_vector) != 90:\n",
    "#             # Generate synthetic feature vector if missing\n",
    "#             # In production, this would come from Component 4\n",
    "#             print(f\"‚ö†Ô∏è Generating synthetic feature vector for memory {memory.get('id', 'unknown')}\")\n",
    "            \n",
    "#             # Create features based on available metadata\n",
    "#             importance = memory.get('importance_score', 0.5)\n",
    "#             emotional = memory.get('emotional_significance', 0.5)\n",
    "#             temporal = memory.get('temporal_relevance', 0.5)\n",
    "#             access_freq = min(memory.get('access_frequency', 0) / 10.0, 1.0)\n",
    "            \n",
    "#             # Memory type encoding (one-hot)\n",
    "#             memory_type = memory.get('memory_type', 'conversation')\n",
    "#             type_encoding = [0.0] * 4\n",
    "#             type_map = {'conversation': 0, 'event': 1, 'emotion': 2, 'insight': 3}\n",
    "#             if memory_type in type_map:\n",
    "#                 type_encoding[type_map[memory_type]] = 1.0\n",
    "            \n",
    "#             # Content features (synthetic)\n",
    "#             content_length = len(memory.get('content_summary', '')) / 500.0\n",
    "#             content_features = [content_length] * 10\n",
    "            \n",
    "#             # Temporal features\n",
    "#             try:\n",
    "#                 created_at = datetime.fromisoformat(memory.get('created_at', '2024-01-01'))\n",
    "#                 days_old = (datetime.now() - created_at).days / 365.0  # Normalize to years\n",
    "#             except:\n",
    "#                 days_old = 0.5\n",
    "            \n",
    "#             temporal_features = [days_old] * 5\n",
    "            \n",
    "#             # Relationship features\n",
    "#             relationship_count = len(memory.get('relationships', [])) / 10.0\n",
    "#             relationship_features = [relationship_count] * 5\n",
    "            \n",
    "#             # Combine all features to make 90 dimensions\n",
    "#             feature_vector = (\n",
    "#                 [importance, emotional, temporal, access_freq] +  # 4\n",
    "#                 type_encoding +  # 4\n",
    "#                 content_features +  # 10\n",
    "#                 temporal_features +  # 5\n",
    "#                 relationship_features +  # 5\n",
    "#                 [0.5] * 62  # Padding to reach 90 dimensions\n",
    "#             )\n",
    "            \n",
    "#             # Ensure exactly 90 dimensions\n",
    "#             feature_vector = feature_vector[:90]\n",
    "#             while len(feature_vector) < 90:\n",
    "#                 feature_vector.append(0.5)\n",
    "        \n",
    "#         return feature_vector\n",
    "    \n",
    "#     async def process_memory_through_gates(self, memory: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#         \"\"\"Process a single memory through LSTM gates\"\"\"\n",
    "        \n",
    "#         # Ensure valid feature vector\n",
    "#         feature_vector = self._ensure_feature_vector(memory)\n",
    "        \n",
    "#         # Get gate decisions\n",
    "#         gate_decisions = self.gate_network.get_gate_decisions(feature_vector)\n",
    "        \n",
    "#         # Update memory with gate scores\n",
    "#         processed_memory = memory.copy()\n",
    "#         processed_memory['gate_decisions'] = gate_decisions\n",
    "#         processed_memory['feature_vector'] = feature_vector\n",
    "        \n",
    "#         # Update importance score based on input gate\n",
    "#         input_score = gate_decisions['input']['score']\n",
    "#         processed_memory['importance_score'] = input_score\n",
    "        \n",
    "#         # Apply forget gate (memory decay)\n",
    "#         forget_score = gate_decisions['forget']['score']\n",
    "#         if gate_decisions['forget']['decision']:\n",
    "#             # Apply decay based on forget gate\n",
    "#             current_importance = processed_memory['importance_score']\n",
    "#             processed_memory['importance_score'] = current_importance * (1 - forget_score)\n",
    "#             self.stats['forget_gate_triggered'] += 1\n",
    "        \n",
    "#         # Track input gate decisions\n",
    "#         if gate_decisions['input']['decision']:\n",
    "#             self.stats['input_gate_accepted'] += 1\n",
    "        \n",
    "#         # Track output gate decisions\n",
    "#         if not gate_decisions['output']['decision']:\n",
    "#             self.stats['output_gate_filtered'] += 1\n",
    "        \n",
    "#         return processed_memory\n",
    "    \n",
    "#     async def filter_memories_by_gates(self, memories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"Filter memories based on gate decisions\"\"\"\n",
    "        \n",
    "#         filtered_memories = []\n",
    "        \n",
    "#         for memory in memories:\n",
    "#             gate_decisions = memory.get('gate_decisions', {})\n",
    "            \n",
    "#             # Apply input gate filter (for new memories)\n",
    "#             input_decision = gate_decisions.get('input', {}).get('decision', True)\n",
    "            \n",
    "#             # Apply output gate filter (for retrieval)\n",
    "#             output_decision = gate_decisions.get('output', {}).get('decision', True)\n",
    "            \n",
    "#             # Keep memory if it passes both input and output gates\n",
    "#             if input_decision and output_decision:\n",
    "#                 # Additional importance threshold check\n",
    "#                 importance = memory.get('importance_score', 0.0)\n",
    "#                 if importance > 0.1:  # Minimum importance threshold\n",
    "#                     filtered_memories.append(memory)\n",
    "        \n",
    "#         return filtered_memories\n",
    "    \n",
    "#     async def rank_memories_by_importance(self, memories: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"Rank memories by importance score and relevance\"\"\"\n",
    "        \n",
    "#         # Sort by importance score (descending)\n",
    "#         ranked_memories = sorted(\n",
    "#             memories, \n",
    "#             key=lambda m: m.get('importance_score', 0.0), \n",
    "#             reverse=True\n",
    "#         )\n",
    "        \n",
    "#         # Add ranking metadata\n",
    "#         for i, memory in enumerate(ranked_memories):\n",
    "#             memory['rank'] = i + 1\n",
    "#             memory['percentile'] = (len(ranked_memories) - i) / len(ranked_memories)\n",
    "        \n",
    "#         return ranked_memories\n",
    "    \n",
    "#     async def process_user_memories(self, user_id: str) -> Dict[str, Any]:\n",
    "#         \"\"\"Complete processing pipeline for user memories\"\"\"\n",
    "#         start_time = datetime.now()\n",
    "        \n",
    "#         print(f\"\\nüîÑ Processing Component 5 Pipeline for user: {user_id}\")\n",
    "#         print(\"=\" * 60)\n",
    "        \n",
    "#         try:\n",
    "#             # Step 1: Retrieve memories from Astra DB\n",
    "#             print(\"\\nüìä Step 1: Retrieving memories from Astra DB...\")\n",
    "#             raw_memories = await self.retrieve_user_memories(user_id)\n",
    "            \n",
    "#             if not raw_memories:\n",
    "#                 print(\"‚ö†Ô∏è No memories found for user\")\n",
    "#                 return {\n",
    "#                     'user_id': user_id,\n",
    "#                     'processed_memories': [],\n",
    "#                     'statistics': self.stats,\n",
    "#                     'processing_time': 0.0\n",
    "#                 }\n",
    "            \n",
    "#             # Step 2: Process each memory through LSTM gates\n",
    "#             print(f\"\\nüß† Step 2: Processing {len(raw_memories)} memories through LSTM gates...\")\n",
    "#             processed_memories = []\n",
    "            \n",
    "#             for i, memory in enumerate(raw_memories):\n",
    "#                 print(f\"   Processing memory {i+1}/{len(raw_memories)}: {memory.get('id', 'unknown')[:8]}...\")\n",
    "#                 processed_memory = await self.process_memory_through_gates(memory)\n",
    "#                 processed_memories.append(processed_memory)\n",
    "#                 self.stats['memories_processed'] += 1\n",
    "            \n",
    "#             # Step 3: Filter memories based on gate decisions\n",
    "#             print(f\"\\nüîç Step 3: Filtering memories based on gate decisions...\")\n",
    "#             filtered_memories = await self.filter_memories_by_gates(processed_memories)\n",
    "#             print(f\"   Filtered to {len(filtered_memories)} memories\")\n",
    "            \n",
    "#             # Step 4: Rank memories by importance\n",
    "#             print(f\"\\nüìà Step 4: Ranking memories by importance...\")\n",
    "#             final_memories = await self.rank_memories_by_importance(filtered_memories)\n",
    "#             self.stats['final_memories_count'] = len(final_memories)\n",
    "            \n",
    "#             # Calculate processing time\n",
    "#             processing_time = (datetime.now() - start_time).total_seconds()\n",
    "#             self.stats['processing_time'] = processing_time\n",
    "            \n",
    "#             # Generate summary\n",
    "#             print(f\"\\n‚úÖ Component 5 Processing Complete!\")\n",
    "#             print(f\"   üìä Input memories: {len(raw_memories)}\")\n",
    "#             print(f\"   üß† Processed: {len(processed_memories)}\")\n",
    "#             print(f\"   üîç Filtered: {len(filtered_memories)}\")\n",
    "#             print(f\"   üìà Final output: {len(final_memories)}\")\n",
    "#             print(f\"   ‚è±Ô∏è Processing time: {processing_time:.2f}s\")\n",
    "            \n",
    "#             return {\n",
    "#                 'user_id': user_id,\n",
    "#                 'processed_memories': final_memories,\n",
    "#                 'raw_memories': raw_memories,\n",
    "#                 'gate_statistics': {\n",
    "#                     'input_gate_acceptance_rate': self.stats['input_gate_accepted'] / max(1, self.stats['memories_processed']),\n",
    "#                     'forget_gate_trigger_rate': self.stats['forget_gate_triggered'] / max(1, self.stats['memories_processed']),\n",
    "#                     'output_gate_filter_rate': self.stats['output_gate_filtered'] / max(1, self.stats['memories_processed']),\n",
    "#                     'overall_retention_rate': len(final_memories) / max(1, len(raw_memories))\n",
    "#                 },\n",
    "#                 'statistics': self.stats,\n",
    "#                 'processing_time': processing_time\n",
    "#             }\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error in Component 5 processing: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             return {\n",
    "#                 'user_id': user_id,\n",
    "#                 'error': str(e),\n",
    "#                 'processed_memories': [],\n",
    "#                 'statistics': self.stats\n",
    "#             }\n",
    "    \n",
    "#     def save_results(self, results: Dict[str, Any], filename: str = None):\n",
    "#         \"\"\"Save processing results to JSON file\"\"\"\n",
    "#         if not filename:\n",
    "#             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#             filename = f\"component5_output_{results['user_id']}_{timestamp}.json\"\n",
    "        \n",
    "#         # Convert datetime objects to strings for JSON serialization\n",
    "#         serializable_results = self._make_json_serializable(results)\n",
    "        \n",
    "#         with open(filename, 'w') as f:\n",
    "#             json.dump(serializable_results, f, indent=2)\n",
    "        \n",
    "#         print(f\"üíæ Results saved to: {filename}\")\n",
    "#         return filename\n",
    "    \n",
    "#     def _make_json_serializable(self, obj):\n",
    "#         \"\"\"Convert objects to JSON serializable format\"\"\"\n",
    "#         if isinstance(obj, dict):\n",
    "#             return {k: self._make_json_serializable(v) for k, v in obj.items()}\n",
    "#         elif isinstance(obj, list):\n",
    "#             return [self._make_json_serializable(item) for item in obj]\n",
    "#         elif isinstance(obj, datetime):\n",
    "#             return obj.isoformat()\n",
    "#         elif hasattr(obj, '__dict__'):\n",
    "#             return self._make_json_serializable(obj.__dict__)\n",
    "#         else:\n",
    "#             return obj\n",
    "\n",
    "\n",
    "# async def main():\n",
    "#     \"\"\"Main execution function\"\"\"\n",
    "#     print(\"üöÄ Component 5 LSTM Memory Gates Pipeline\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     # Initialize pipeline\n",
    "#     pipeline = Component5Pipeline()\n",
    "#     await pipeline.initialize()\n",
    "    \n",
    "#     # Process memories for user_id = \"123\"\n",
    "#     user_id = \"123\"\n",
    "#     results = await pipeline.process_user_memories(user_id)\n",
    "    \n",
    "#     # Save results\n",
    "#     output_file = pipeline.save_results(results)\n",
    "    \n",
    "#     # Display summary\n",
    "#     print(f\"\\nüìã Processing Summary:\")\n",
    "#     print(f\"   User ID: {results['user_id']}\")\n",
    "#     print(f\"   Memories processed: {results['statistics']['memories_processed']}\")\n",
    "#     print(f\"   Final memories: {results['statistics']['final_memories_count']}\")\n",
    "#     print(f\"   Processing time: {results['statistics']['processing_time']:.2f}s\")\n",
    "    \n",
    "#     if 'gate_statistics' in results:\n",
    "#         gate_stats = results['gate_statistics']\n",
    "#         print(f\"\\nüß† Gate Statistics:\")\n",
    "#         print(f\"   Input gate acceptance: {gate_stats['input_gate_acceptance_rate']:.1%}\")\n",
    "#         print(f\"   Forget gate trigger: {gate_stats['forget_gate_trigger_rate']:.1%}\")\n",
    "#         print(f\"   Output gate filter: {gate_stats['output_gate_filter_rate']:.1%}\")\n",
    "#         print(f\"   Overall retention: {gate_stats['overall_retention_rate']:.1%}\")\n",
    "    \n",
    "#     print(f\"\\nüíæ Output saved to: {output_file}\")\n",
    "#     print(f\"üéØ Ready for Component 6 integration!\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Run the pipeline\n",
    "#     results = asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
